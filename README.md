ğŸ¤– HR Policy Assistant Chatbot
An intelligent HR assistant powered by Weaviate, Flask, and Gradio that enables semantic search and natural language question-answering over HR policy PDF documents using LLMs (via Ollama) and vector embeddings.

ğŸš€ Features
ğŸ“„ Upload HR policy documents (PDF)

ğŸ” Perform semantic search on document chunks

ğŸ’¬ Ask HR-related queries in natural language

ğŸ¤– LLM-generated answers based on document context

ğŸ§  Uses Weaviate's text2vec-ollama and generative-ollama modules

ğŸŒ Simple web-based UI using Gradio

ğŸ“ Project Structure
bash
Copy
Edit
HR-Policy-Assistant-Chatbot/
â”‚
â”œâ”€â”€ backend.py            # Flask backend handling upload, search, and chat
â”œâ”€â”€ chunks.py             # PDF text extraction and chunking logic
â”œâ”€â”€ connection.py         # Weaviate connection and collection setup
â”œâ”€â”€ frontend.py           # Gradio interface
â”œâ”€â”€ docker-compose.yml    # Weaviate local setup
â”œâ”€â”€ .env                  # Environment variables for model endpoints
â””â”€â”€ README.md             # You're here!
ğŸ› ï¸ Technologies Used
Python 3.10+

Flask

Gradio

PyPDF2

LangChain

Weaviate (with Ollama for vectorization & LLMs)

Docker (for Weaviate deployment)

âš™ï¸ Setup Instructions
1. Clone the Repository
bash
Copy
Edit
git clone https://github.com/yourusername/hr-policy-assistant-chatbot.git
cd hr-policy-assistant-chatbot
2. Set up the Environment
Install Python dependencies:

bash
Copy
Edit
pip install -r requirements.txt
Create a .env file:

env
Copy
Edit
DOCKER_API_ENDPOINT=http://localhost:8080
VECTORIZER=llama2
LLM=llama2
Ensure the model names match your local Ollama setup (e.g., LLaMA2, Mistral, etc.)

3. Run Weaviate
Start Weaviate via Docker Compose:

bash
Copy
Edit
docker-compose up -d
4. Run Backend Server
bash
Copy
Edit
python backend.py
5. Launch the Frontend
bash
Copy
Edit
python frontend.py
Gradio UI will open in your browser at: http://localhost:7860

ğŸ§ª How It Works
Upload PDF: The document is parsed and split into text chunks.

Data Insertion: Chunks are stored in a Weaviate collection with embeddings.

Semantic Query: The user's question is matched with the most relevant chunk.

LLM Answer: The top chunk is used as context to generate a helpful answer.
